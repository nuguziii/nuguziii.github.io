<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-10-03T21:46:36+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ZZEN’s Blog</title><subtitle>Hello! ZZEN의 개발 블로그입니다 :&gt;</subtitle><author><name>ZZEN</name><email>nuguziii@naver.com</email></author><entry><title type="html">[논문읽기] A Comprehensive Overhaul of Feature Distillation</title><link href="http://localhost:4000/paper%20review/PR-005/" rel="alternate" type="text/html" title="[논문읽기] A Comprehensive Overhaul of Feature Distillation" /><published>2019-10-03T00:00:00+09:00</published><updated>2019-10-03T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/PR-005</id><content type="html" xml:base="http://localhost:4000/paper%20review/PR-005/">&lt;p&gt;이 글은 2019 ICCV 논문, &lt;a href=&quot;https://arxiv.org/pdf/1904.01866.pdf&quot;&gt;A Comprehensive Overhaul of Feature Distillation&lt;/a&gt;과 &lt;a href=&quot;https://clova-ai.blog/2019/08/22/a-comprehensive-overhaul-of-feature-distillation-iccv-2019/&quot;&gt;clova-ai blog&lt;/a&gt;를 참고하여 작성하였습니다.&lt;/p&gt;

&lt;p&gt;knowledge distillation 분야에 관심있어서 막 공부를 시작하는 사람들이 읽었으면 좋겠다고 생각하여 소개하게 된 논문입니다.&lt;/p&gt;

&lt;p&gt;저도 knowledge distillation에 대한 대략적인 개념만 알고 최근 어떤 연구들이 있는지 찾아보던 중이었는데, 이 논문에서 지금까지 이루어진 distillation 연구들을 잘 정리해놓아서 공부하는데 큰 도움이 되었습니다.&lt;/p&gt;

&lt;p&gt;기존 knowledge distillation 연구들과 이 논문에서 제안한 컨셉 위주로 논문을 리뷰해보도록 하겠습니다:&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;스마트폰과 같은 가벼운 기기에서 딥러닝 모델이 돌아갈 수 있도록 하기 위해서는 모델 사이즈가 작아야합니다. 이를 위해 모델을 더 작고 비용효율적으로 만들어야 하는데, 이를 위한 연구가 Knowledge Distillation (KD) 입니다.&lt;/p&gt;

&lt;p&gt;KD에서는 smaller network (student) 와 larger network (teacher)를 이용해 teacher network의 knowledge를 student network에 잘 옮겨서 비슷한 성능이 나오게 합니다.&lt;/p&gt;

&lt;p&gt;이때, “어떠한 방법을 이용하는가” 이것이 매우 중요합니다. 이 논문에서는 기존 연구들에서 사용했던 “teacher와 student network의 차이를 줄이는 방법”을 매우 잘 정리해놓았습니다.&lt;/p&gt;

&lt;p&gt;teacher network와 student network의 output의 차이를 좁히는 방법 만으로는 부족하여 “feature distillation”이 있습니다. hidden feature들도 비슷하게끔 하는거죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/1ein6kc.png&quot; alt=&quot;structure&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;font-size:10px;text-align:center&quot;&gt;general training scheme&lt;/p&gt;

&lt;p&gt;위에 그림처럼 중간중간 hidden feature 사이의 distance를 구하여 학습하는 방법이 일반적입니다. 이때 각 T와 d를 무엇을 쓰는지에 따라 성능이 달라집니다. 식으로 나타내면 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{distill} = d(T_t(F_t), T_s(F_s))&lt;/script&gt;

&lt;p&gt;이 loss를 최소화하는 것이 목표입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/v2VYyu6.png&quot; alt=&quot;difference&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존 논문과 이 논문에서 &lt;script type=&quot;math/tex&quot;&gt;T_t, T_s&lt;/script&gt;와 distance 등을 구하기 위해 어떠한 방법을 썼는지 정리해놓은 표입니다. 이 논문에서는 Teacher transform, disillation feature position, distance 등에 다른 방법 썼습니다.&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Transfer all feature information without missing any import information from the teacher”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 논문에서는 최대한 정보손실이 일어나지 않는 방향으로 distillation loss를 설계했습니다. 논문에서 제안한 방법은 다음 그림과 같이 정리할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i2.wp.com/clova-ai.blog/wp-content/uploads/2019/08/ourmethod.png?resize=768%2C432&amp;amp;ssl=1&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;font-size:10px;text-align:center&quot;&gt;propsed method&lt;/p&gt;

&lt;h3 id=&quot;--margin-relu&quot;&gt;- Margin ReLU&lt;/h3&gt;

&lt;p&gt;margin ReLU를 사용함으로써 도움이 되는 정보들(positive information)은 변형 없이 사용되고, 도움이 되지 않는 정보들(negative information)은 사용되지 않습니다. 이에 따라 beneficial information 중에 빠뜨린 것 없이 모두 distillation 될 수 있게 됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_m(x) = max(x, m).&lt;/script&gt;

&lt;p&gt;따라서 다음과 같은 식을 사용하는데요, 이때 m은 음수로, positive value들은 student가 teacher과 같은 값을 가질 수 있도록 하고, negative value에 대해서는 0보다 작은 값을 나오게 하여 student value도 음수로 만들게 됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
m_c = E[F_t^i\mid F_t^i&lt;0, i \in C]. %]]&gt;&lt;/script&gt;

&lt;p&gt;이때, margin m을 구하는 방법은 위 식과 같습니다. m은 channel-by-channel 음수 값들의 기대값입니다.&lt;/p&gt;

&lt;h3 id=&quot;--partial-l2&quot;&gt;- Partial L2&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
d_p(T,S) = \sum_i^{WHC}
\begin{cases}
0 &amp; {if \ \ } {S_i\le T_i\le 0 }\\
(T_i-S_i)^2 &amp; {otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;student가 teacher 보다 작고, 음수인 경우를 제외하고 L2를 구해줍니다. 이는, teacher의 negative response에 대해서는 student value가 target value 보다 크면 반드시 감소해야하고, 그렇지 않은 경우에는 그럴 필요가 없기 때문입니다.&lt;/p&gt;

&lt;h3 id=&quot;--distillation-loss&quot;&gt;- Distillation Loss&lt;/h3&gt;

&lt;p&gt;따라서 위를 적용한 최종적인 distillation loss 식은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{distill} = d_p(\sigma_{m_c}(F_t), r(F_s))&lt;/script&gt;

&lt;hr /&gt;
&lt;p style=&quot;font-size:14px; color:gray;&quot;&gt;
오늘은 기존 distillation 연구들과 이 논문에서 제안한 방법 위주로 간단하게 글을 작성해보았습니다. 매우 짧게 끝난 것 같지만, 요즘 시간이 없어서ㅠㅠ 짧게나마 리뷰해봅니다.
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;br /&gt;
[1] &lt;a href=&quot;https://arxiv.org/pdf/1904.01866.pdf&quot;&gt;paper&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>ZZEN</name><email>nuguziii@naver.com</email></author><category term="Knowledge Distillation" /><summary type="html">이 글은 2019 ICCV 논문, A Comprehensive Overhaul of Feature Distillation과 clova-ai blog를 참고하여 작성하였습니다.</summary></entry><entry><title type="html">[논문읽기] Image Super-Resolution by Neural Texture Transfer(SRNTT)</title><link href="http://localhost:4000/paper%20review/PR-004/" rel="alternate" type="text/html" title="[논문읽기] Image Super-Resolution by Neural Texture Transfer(SRNTT)" /><published>2019-09-10T00:00:00+09:00</published><updated>2019-09-10T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/PR-004</id><content type="html" xml:base="http://localhost:4000/paper%20review/PR-004/">&lt;p&gt;이 글은 2019 CVPR 논문, &lt;a href=&quot;https://arxiv.org/abs/1903.00834&quot;&gt;Image Super-Resolution by Neural Texture Transfer&lt;/a&gt;를 참고하여 작성하였습니다.&lt;/p&gt;

&lt;p&gt;Image Denoising 관련된 논문 &lt;a href=&quot;https://nuguziii.github.io/paper%20review/PR-002/&quot;&gt;Noise2Noise&lt;/a&gt;을 읽은 적이 있었는데, 이번에는 super-resolution은 영상의 해상도를 더 좋게하는 것입니다. 대표적인 super-resolution 논문으로는 2016년 논문인 &lt;a href=&quot;(https://arxiv.org/abs/1609.04802)&quot;&gt;SRGAN&lt;/a&gt; 있습니다. perceptual loss로 GAN을 사용해서 영상의 texture를 잘 살린 논문이었습니다.&lt;/p&gt;

&lt;p&gt;오늘 소개할 논문은 reference-based super-resolution으로 reference를 사용하여 아래 그림처럼(SRNTT) SRGAN 보다 더 좋은 성능을 보이고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/MFzkdau.jpg&quot; alt=&quot;xx&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 한번 어떠한 방법을 썼는지 한번 자세히 살펴보도록 하겠습니다:)&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Super Resolution은 저해상도 영상을 이용해 고해상도 영상을 되찾는 방법입니다. single image super resolution은 SISR로 부릅니다. 이때 고해상도 영상을 되살리는 과정에서 texture가 약간의 blurry하게 잘 드러나지 않는다는 문제점들이 있습니다.(그래서 이 문제를 SRGAN 논문에서는 perceptual loss를 사용하여 보완하였습니다.)&lt;/p&gt;

&lt;p&gt;이러한 texture를 더 잘 살리고자 하는 방법으로 reference-based super-resolution(RefSR)도 있습니다. 모델이 주어진 저해상도 이미지에서 texture를 살리기는 어려우니, 참고될 만한 영상도 같이 제공하여서 texture를 더 잘 살릴 수 있도록 돕는 것이죠.&lt;/p&gt;

&lt;p&gt;이 논문에서는 reference image들이 주어진 저해상도 이미지랑 관련이 있던 없던 기존 SISR 모델들보다 높은 성능을 보인다고 합니다. 관련이 있으면 훨씬 좋은 성능을, 관련이 없는 이미지라도 성능이 떨어지지 않고 기존 모델들과 비슷한 성능을 보입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;adaptively transfers textures from the Ref images to the SR image&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;만약, 의미적으로 Ref image와 SR image가 연관이 있다면 Ref image의 텍스쳐를 transfer 하는 방식을 이용합니다.&lt;/p&gt;

&lt;p&gt;주요 contribution은 다음과 같습니다.&lt;/p&gt;

&lt;form&gt;
  &lt;fieldset&gt;
  &lt;div&gt;1. SISR보다 더 나은 성능 (lack of texture detail), RefSR의 문제점 개선&lt;/div&gt;&lt;br /&gt;
  &lt;div&gt;2. SRNTT end-to-end deep model로 multi-scale neural texture transfer를 이용해 어떤 reference가 주어져도 SR 가능&lt;/div&gt;&lt;br /&gt;
  &lt;div&gt;3. dataset CUFED5&lt;/div&gt;
  &lt;/fieldset&gt;
&lt;/form&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;우선, 이 논문에서 제안한 전체적인 구조를 보시면 아래 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/TMA6jNm.png&quot; alt=&quot;스크린샷 2019-09-10 오후 2.11.00&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ref image에서 쓸만한 정보들을 뽑는 부분(Feature Swapping-노란색 부분), low resolution image에 쓸만한 정보들을 옮기며 해상도를 높이는 부분(Texture Transfer-파란색 부분) 이렇게 두 부분으로 나뉘어 있습니다. 여기서 쓸만한 정보들은 texture 가 되겠죠.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;I^{SR}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;I^{LR}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt;가 있을 때, feature space에서 &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt;와 매칭되는 texture를 찾고 (Feature Swapping),
매칭된 texture를 multi-scale 방식을 이용해 &lt;script type=&quot;math/tex&quot;&gt;I^{SR}&lt;/script&gt;로 transfer 한다. (Texture Transfer)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;feature-swapping&quot;&gt;Feature Swapping&lt;/h3&gt;

&lt;p&gt;Feature Swapping에서는 &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt; 영상의 모든 부분을 탐색하면서 부분적으로 비슷한 텍스쳐가 있는지 찾아냅니다. 이때, &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;I^{LR}&lt;/script&gt; 사이에 매칭되는 texture가 있는지 판단하기 위해 inner product를 사용합니다.&lt;/p&gt;

&lt;p&gt;j번째 ref patch와 &lt;script type=&quot;math/tex&quot;&gt;I^{LR}&lt;/script&gt; 사이의 inner product를 계산하여 similarity map &lt;script type=&quot;math/tex&quot;&gt;S_j&lt;/script&gt;를 구하는 식은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_j = \phi(I^{LR\uparrow}) * {P_j(\phi(I^{Ref\downarrow\uparrow}))}\over{ \mid\mid P_j(\phi(I^{Ref\downarrow\uparrow}))\mid\mid}&lt;/script&gt;

&lt;p&gt;이때, &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;I^{LR}&lt;/script&gt;의 해상도를 맞춰주기 위해  &lt;script type=&quot;math/tex&quot;&gt;I^{LR}&lt;/script&gt; 은 up-sampling을 해주고, &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt;는 up-sampling과 down-sampling을 해주어 blurry하게 만듭니다. feature space &lt;script type=&quot;math/tex&quot;&gt;\phi(I)&lt;/script&gt;에서 유사도 매칭을 합니다.&lt;/p&gt;

&lt;p&gt;구한 similarity map에서 높은 score를 갖는 부분들을 찾아서 transfer 시켜주기 위하여 swapped feature map M을 구합니다. 식은 아래와 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{w(x,y)}(M) = P_j * (\phi(I^{Ref})), j* = \arg \underset {j}{max}S_j(x,y)&lt;/script&gt;

&lt;p&gt;이때는 &lt;script type=&quot;math/tex&quot;&gt;I^{Ref \downarrow \uparrow}&lt;/script&gt; 가 아닌 &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt;를 사용하여 구합니다.&lt;/p&gt;

&lt;h3 id=&quot;neural-texture-transfer&quot;&gt;Neural Texture Transfer&lt;/h3&gt;

&lt;p&gt;위의 Feature Swapping 과정을 통해 나온 swapped feature map은 Neural Texture Transfer (전체 구조의 파란 부분)에 사용됩니다.&lt;/p&gt;

&lt;p&gt;layer와 layer 사이의 자세한 구조는 아래 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/0OxvPSs.png&quot; alt=&quot;texture_transfer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 layer에서 나온 output과 swapped feature map을 concatenate하여 residual block에 넣어주고, 이 결과를 output과 다시 더해줍니다. 이 과정을 계속 반복하며 점차 타겟인 HR resoultion에 도달하게 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;loss&quot;&gt;Loss&lt;/h3&gt;

&lt;p&gt;이 논문에서는 training을 위해 4가지 Loss를 사용합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = L_{rec} + L_{per} + L_{adv} + L_{tex}&lt;/script&gt;

&lt;p&gt;이때 reconstruction loss로는 MSE, perceptual loss는 VGG19의 relu5_1 layer를, adversarial loss로는 WGAN을 사용하였습니다.&lt;/p&gt;

&lt;p&gt;그리고 이 논문에서는 SISR과 다르게 &lt;script type=&quot;math/tex&quot;&gt;I^{SR}&lt;/script&gt;과 &lt;script type=&quot;math/tex&quot;&gt;I^{Ref}&lt;/script&gt;사이의 texture difference도 고려하기 위해 loss &lt;script type=&quot;math/tex&quot;&gt;L_{tex}&lt;/script&gt;를 정의합니다.(&lt;script type=&quot;math/tex&quot;&gt;I^{SR}&lt;/script&gt;이 &lt;script type=&quot;math/tex&quot;&gt;M_l&lt;/script&gt;과 비슷해지기 위함)&lt;/p&gt;

&lt;p&gt;식은 아래와 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{tex} = \sum_{l}\lambda_l{||Gr(\phi_l(I^{SR})\centerdot S_l^\star) - Gr(M_l \centerdot S_l^\star)||}_F&lt;/script&gt;

&lt;p&gt;여기에서는 Gram matrix를 사용하였고, &lt;script type=&quot;math/tex&quot;&gt;S_l^{\star}&lt;/script&gt;(weighting map for all LR patches)를 사용하여 penalty를 줍니다.&lt;/p&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;p&gt;이 논문에서는 CUFED5 데이터셋을 사용해서 다양한 similarity level에 따른 Ref image들을 정해줍니다. 4가지의 similarity level을 정의했고, 아래 그림에서 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/wr17UDh.png&quot; alt=&quot;CUFED5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 왼쪽이 HR image이고, 그 옆에 차례대로 L1, L2, L3, L4 level 데이터셋입니다.&lt;/p&gt;

&lt;p&gt;기존의 Super Resolution 논문들에서 제안한 방법과 비교한 그림입니다. SRNTT가 SRGAN과 비교했을 때도 성능이 좋고 같은 방법 (reference-based)을 사용한 CrossNet과 비교해도 훨씬 좋은 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/IQm1aBA.jpg&quot; alt=&quot;캡처&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;font-size:14px; color:gray;&quot;&gt;
multi-scale transfer 방법으로 이전 논문보다 더 좋은 성과를 낼 수 있었던 것 같습니다. 8월 달에 바쁘다 보니 한 달 동안 글을 못썼는데, 한번 다시 열심히 써보도록 하겠습니다.
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;br /&gt;
[1] &lt;a href=&quot;http://web.eecs.utk.edu/~zzhang61/project_page/SRNTT/cvpr2019_final.pdf&quot;&gt;Paper&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://github.com/ZZUTK/SRNTT&quot;&gt;SRNTT github&lt;/a&gt;&lt;/p&gt;</content><author><name>ZZEN</name><email>nuguziii@naver.com</email></author><category term="Vision" /><summary type="html">이 글은 2019 CVPR 논문, Image Super-Resolution by Neural Texture Transfer를 참고하여 작성하였습니다.</summary></entry><entry><title type="html">[논문읽기] Noise2Noise: Learning Image Restoration without Clean Data</title><link href="http://localhost:4000/paper%20review/PR-002/" rel="alternate" type="text/html" title="[논문읽기] Noise2Noise: Learning Image Restoration without Clean Data" /><published>2019-08-02T00:00:00+09:00</published><updated>2019-08-02T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/PR-002</id><content type="html" xml:base="http://localhost:4000/paper%20review/PR-002/">&lt;p&gt;이 글은 2018 ICML 논문, &lt;a href=&quot;https://arxiv.org/pdf/1803.04189.pdf&quot;&gt;Noise2Noise: Learning Image Restoration without Clean Data&lt;/a&gt;와 &lt;a href=&quot;https://vimeo.com/287807759&quot;&gt;TechTalksTV 영상&lt;/a&gt;을 참고하여 작성하였습니다.&lt;/p&gt;

&lt;p&gt;이 논문은 작년에 발표되었던 논문이지만, 읽었을 당시 매우 인상깊어서 꼭 블로그 글로 작성해보고 싶었습니다. Image Denoising 주제에 대해 관심이 많았었는데, 기존 논문들과 다른 방식으로 접근했던 점이 인상깊었습니다. 대부분의 Image Denoising Task 들은 supervised learning 으로 clean한 영상을 학습하기 위해서 noisy 영상과 original 영상을 모두 필요로 합니다. 하지만, 이 논문에서는 약간 다른 방법을 씁니다. 오로지 noisy한 영상만으로 clean한 영상을 뽑아내죠. clean한 영상이 없을 경우에 고민할 필요가 없어졌습니다.&lt;/p&gt;

&lt;p&gt;논문에도 다음과 같이 적혀있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“learn to turn bad images into good images by only looking at bad images”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그럼 한번 Noise2Noise 논문 리뷰 시작해보도록 하겠습니다:)&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;일반적인 Image Denoising Task에서 식은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg\min\sum_{i}L(f_\theta(\hat{x_i}), y_i)&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{x_i}&lt;/script&gt;는 corrupted image (noisy image)이고, &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;는 clean image (target)입니다. 둘 사이에 Loss Function (L1 또는 L2)을 통해 차이를 minimize 시키고, noise가 제거되도록 학습됩니다. 대표적으로 &lt;a href=&quot;https://arxiv.org/abs/1608.03981&quot;&gt;DnCNN&lt;/a&gt; 논문에서 이러한 방식을 씁니다. (다만, DnCNN에서는 &lt;script type=&quot;math/tex&quot;&gt;f_\theta(\hat{x_i})&lt;/script&gt; 값이 denoised image가 아닌 residual이니 noisy image에서 빼줘야합니다) 하지만, 이러한 방법으로 학습하기 위해서는 항상 Clean Image를 필요로 합니다. Clean training target들을 얻는 것은 때론 어렵거나 cost가 큽니다. (ex. long exposure)&lt;/p&gt;

&lt;p&gt;또한, 논문에서는 L1/L2 Loss에 대해서도 자세히 설명을 하고 있습니다. L1 Loss는 median값을 추정하고, L2 Loss는 mean 값을 추정합니다. (이유에 대해서는 &lt;a href=&quot;https://stats.stackexchange.com/questions/34613/l1-regression-estimates-median-whereas-l2-regression-estimates-mean&quot;&gt;여기&lt;/a&gt;에 수학적/직관적인 여러 설명들이 있습니다.) Image Denoising을 할 때를 생각해보면, 보통 input과 target간의 1:1 mapping이 아니고, target이 될 수 있는 것은 여러가지 입니다.(noisy image에서 노이즈를 제거했을 때 나오는 최적의 clean image는 하나만 있을수가 없습니다) 따라서, 여기에 L2 Loss를 적용한다면 네트워크는 모든 가능한 target의 경우를 평균낸 결과를 학습하고, 결과에 blurriness 효과가 있을 수 밖에 없습니다. (perceptual loss를 쓰는 이유)&lt;/p&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;이 논문에서는 L2 Loss가 모든 가능한 target의 경우를 평균낸다는 점을 이용합니다. 모든 가능한 경우를 평균낸 최적의 output z (denoised image)를 식으로 나타내면 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = \mathbb{E_y}\{y\}&lt;/script&gt;

&lt;p&gt;하지만, 이때 y가 항상 clean image일 필요는 없습니다. 각 y가 random numbers여도 expectation만 target과 일치한다면 같은 방법으로 최적의 z를 찾을 수 있겠죠. 바로 이것이 논문이 제안한 방법입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This implies that we can, in principle, corrupt the training targets of a neural newtork with zero-mean noise without changing what the network learns&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;식을 이제는 이렇게 쓸 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg\min\sum_{i}L(f_\theta(\hat{x_i}), \hat{y_i})&lt;/script&gt;

&lt;p&gt;x와 y가 모두 corrupted image가 되는 것이죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Oreaq8x.png&quot; alt=&quot;캡처&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 그림으로도 논문의 방법을 설명할 수 있습니다. 이 그림은 저자의 발표 영상에서 가져온 것입니다. 왼쪽이 원래 기존에 많이 하는 방법, 오른쪽이 논문에서 제안한 방법입니다. Averge gradient를 통해 주위의 noisy target으로 부터 clean target을 얻게 되는 것을 보여줍니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;논문에서는 gaussian noise, synthesis noise, Monte Carlo Rendering, MRI 등의 noise removal에 대한 실험 결과를 보여주고 있습니다. 다음 그림도 논문 실험결과의 일부인데요, 더 자세한 training detail, result 등은 &lt;a href=&quot;https://arxiv.org/pdf/1803.04189.pdf&quot;&gt;논문&lt;/a&gt;을 참고하면 좋을 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/AHgJmI4.png&quot; alt=&quot;캡처&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p style=&quot;font-size:14px; color:gray;&quot;&gt;
읽었을 당시에는 몰랐는데 리뷰하면서 보니 또 NVIDIA 논문이었네요.:o 직관적으로는 알겠는데, 수학적으로 이해하려니 조금 어려움이 있었던 논문이었습니다. (수학 공부를 더 열심히...) 다음에 기회가 된다면 이 논문에 영감을 받아서 나온 올해 CVPR논문 [Noise2Void](https://arxiv.org/pdf/1811.10980.pdf)도 리뷰해보도록 하겠습니다.
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;:&lt;br /&gt;
[1] &lt;a href=&quot;https://arxiv.org/pdf/1803.04189.pdf&quot;&gt;Noise2Noise paper&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://github.com/NVlabs/noise2noise#noise2noise-learning-image-restoration-without-clean-data---official-tensorflow-implementation-of-the-icml-2018-paper&quot;&gt;Noise2Noise github&lt;/a&gt;&lt;/p&gt;</content><author><name>ZZEN</name><email>nuguziii@naver.com</email></author><category term="Vision" /><summary type="html">이 글은 2018 ICML 논문, Noise2Noise: Learning Image Restoration without Clean Data와 TechTalksTV 영상을 참고하여 작성하였습니다.</summary></entry><entry><title type="html">[논문읽기] Structured Knowledge Distillation for Semantic Segmentation</title><link href="http://localhost:4000/paper%20review/PR-003/" rel="alternate" type="text/html" title="[논문읽기] Structured Knowledge Distillation for Semantic Segmentation" /><published>2019-08-02T00:00:00+09:00</published><updated>2019-08-02T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/PR-003</id><content type="html" xml:base="http://localhost:4000/paper%20review/PR-003/">&lt;p&gt;이 글은 2019 CVPR 논문, &lt;a href=&quot;https://arxiv.org/pdf/1903.04197.pdf&quot;&gt;Structured Knowledge Distillation for Semantic Segmentation
&lt;/a&gt;를 참고하여 작성하였습니다.&lt;/p&gt;

&lt;p&gt;Knowledge distillation은 실제로 서비스에 적용하는 측면에서 굉장히 중요한 부분인 것 같아서 관심이 많은 연구주제 중 하나입니다. knowledge distillation에 대해서는 &lt;a href=&quot;https://medium.com/neuralmachine/knowledge-distillation-dc241d7c2322&quot;&gt;Ujjwal Upadhyay’s Blog&lt;/a&gt;에 정의와 필요성에 대해서 잘 설명되어 있으니 참고하면 좋을 것 같습니다. (+ &lt;a href=&quot;https://arxiv.org/pdf/1503.02531v1.pdf&quot;&gt;Distilling the Knowledge in a Neural Network 논문&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;이 논문에서는 Semantic Segmentation을 위한 Knowledge distillation 방법을 제안합니다. 다음과 같은 3가지 distillation 방법으로 정리할 수 있을 것 같습니다.&lt;/p&gt;

&lt;form&gt;
  &lt;fieldset&gt;
  &lt;div&gt;1. pixel-wise distillation&lt;/div&gt;&lt;br /&gt;
  &lt;div&gt;2. pair-wise distillation&lt;/div&gt;&lt;br /&gt;
  &lt;div&gt;3. holistic distillation&lt;/div&gt;
  &lt;/fieldset&gt;
&lt;/form&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;일단, 이 논문에서 제안한 distillation framework 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/SRGRgxI.png&quot; alt=&quot;캡처&quot; height=&quot;700px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 구조를 간단히 설명하자면, 처음에 Teacher net/Student net에 RGB image가 input으로 들어가고, Feature map이 계산됩니다. (여기서 Teacher network/Student network는 각각 cumbersome network/compact network로도 불립니다.) 이 feature map들은 upsample/classifier를 통해 최종 segmentation map이 나오게 됩니다. 이때 학습을 위해 사용되는 것이 Pixel-wise loss, Pair-wise loss, Holistic loss 입니다. 각각에 대해 더 자세히 알아보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;pixel-wise-distillation&quot;&gt;pixel-wise distillation&lt;/h3&gt;
&lt;p&gt;pixel-wise distillation은 cumbersome network에서 생성된 pixel에 해당하는 class probability를 compact network에 transfer 합니다. 이때 각각의 probability를 transfer 해야하기 때문에 두 확률분포의 차이를 계산하는 데에 사용하는 함수인 쿨백-라이블러 발산(Kullback–Leibler divergence, &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0&quot;&gt;KLD&lt;/a&gt;)을 사용하여 loss를 계산합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{l_{pi}}(S) = {1 \over {W'\times H'}}\sum_{i \in R}KL(q_i^s||q_i^t)&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;q_i^s&lt;/script&gt; compact network S에서 생성된 i번째 픽셀의 class probability이고, &lt;script type=&quot;math/tex&quot;&gt;q_i^t&lt;/script&gt;는 cumbersome network T의 class probability 입니다. 여기서 R은 (1, … , W’xH’)입니다. 둘 사이의 KL을 계산한 것이 loss가 되는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;pair-wise-distillation&quot;&gt;pair-wise distillation&lt;/h3&gt;
&lt;p&gt;semantic segmentation은 structured prediction problem이므로 structure information을 compact network에 transfer 해야합니다. 이때 사용하는 방법이 pair-wise distillation과 밑에 설명드릴 holistic distillation 입니다. 여기서는 픽셀 사이의 pair-wise similarities를 계산합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{l_{pa}}(S) = {1 \over ({W'\times H'})^2}\sum_{i \in R}\sum_{i \in R}(a_{ij}^s - a_{ij}^t)^2&lt;/script&gt;

&lt;p&gt;여기서 &lt;script type=&quot;math/tex&quot;&gt;a_{ij}&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;f_i^Tf_j/({\lVert f_i \rVert}_2{\lVert f_j \rVert}_2)&lt;/script&gt; 식을 통해 구하는데, f는 해당하는 픽셀의 feature를 의미합니다. feature map의 demension을 (W,H,N)이라고 할때, f는 (1,N)입니다. 따라서 &lt;script type=&quot;math/tex&quot;&gt;a_{ij}&lt;/script&gt;값은 scalar가 되겠죠.&lt;/p&gt;

&lt;h3 id=&quot;holistic-distillation&quot;&gt;holistic distillation&lt;/h3&gt;
&lt;p&gt;holistic distillation은 compact network와 cumbersome network의 segmentation map들 사이의 high-order relation을 위해 사용됩니다. 이때, &lt;a href=&quot;https://arxiv.org/pdf/1411.1784.pdf&quot;&gt;conditional generative adversarial learning&lt;/a&gt;의 컨셉과 &lt;a href=&quot;https://arxiv.org/pdf/1701.07875.pdf&quot;&gt;Wasserstein distance&lt;/a&gt;가 사용됩니다. input RGB image I가 주어졌을 때, student network에 의해 생성된 segmentation map &lt;script type=&quot;math/tex&quot;&gt;Q^s&lt;/script&gt;는 fake, teacher network에 의해 생성된 &lt;script type=&quot;math/tex&quot;&gt;Q^t&lt;/script&gt;는 real로 해서 둘 사이의 분포 차이를 계산합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{l_{ho}}(S,D) = \mathbb{E}_{Q^s \sim p_s(Q^s)}[D(Q^s|I)] - \mathbb{E}_{Q^t \sim p_t(Q^t)}[D(Q^t|I)]&lt;/script&gt;

&lt;p&gt;이대 segmentation map과 conditional RGB image는 concatenate 되어서 discriminator input으로 들어갑니다. discriminator는 input image와 segmenation map이 얼마나 잘 매치가 되는지 판단합니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;각 네트워크 구조는 다음과 같습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;teacher network&lt;/th&gt;
      &lt;th&gt;student network&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PSPNet with ResNet101&lt;/td&gt;
      &lt;td&gt;ResNet18, MobileNetV2Plus, MobileNetV2 …&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;데이터셋은 Cityscape, CamVid, ADE20K를 사용하였고, Evaluation Metric으로는 segmentation에서 많이 쓰이는 IoU, model size는 파라미터 수로 측정을 하고, Complexity는 FLOPs로 측정했습니다. 각각에 대한 결과는 &lt;a href=&quot;https://arxiv.org/pdf/1903.04197.pdf&quot;&gt;논문&lt;/a&gt;에 자세히 나와있습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p style=&quot;font-size:14px; color:gray;&quot;&gt;
이전 논문에 비해서는 수학적인 부분이 많이 어렵지 않아서 이해하기 쉬웠던 것 같습니다. conditional generative adversarial learning, WGAN에 대해 더 자세히 공부해보고 싶어졌습니다. (WGAN 언제 공부하지...)
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;br /&gt;
[1] &lt;a href=&quot;https://arxiv.org/pdf/1903.04197.pdf&quot;&gt;논문&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://github.com/irfanICMLL/structure_knowledge_distillation&quot;&gt;논문 코드&lt;/a&gt;&lt;/p&gt;</content><author><name>ZZEN</name><email>nuguziii@naver.com</email></author><category term="Knowledge Distillation" /><category term="Semantic Segmentation" /><summary type="html">이 글은 2019 CVPR 논문, Structured Knowledge Distillation for Semantic Segmentation 를 참고하여 작성하였습니다.</summary></entry><entry><title type="html">[논문읽기] A Style-Based Generator Architecture for Generative Adversarial Networks</title><link href="http://localhost:4000/paper%20review/PR-001/" rel="alternate" type="text/html" title="[논문읽기] A Style-Based Generator Architecture for Generative Adversarial Networks" /><published>2019-08-01T00:00:00+09:00</published><updated>2019-08-01T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/PR-001</id><content type="html" xml:base="http://localhost:4000/paper%20review/PR-001/">&lt;p&gt;이 글은 2019 CVPR 논문, &lt;a href=&quot;https://arxiv.org/abs/1812.04948&quot;&gt;A Style-Based Generator Architecture for Generative Adversarial Networks&lt;/a&gt;를 참고하여 작성하였습니다.&lt;/p&gt;

&lt;p&gt;NVIDIA의 StyleGAN 논문 리뷰로 첫 블로그 글을 작성하게 되었네요.&lt;/p&gt;

&lt;p&gt;이미 올해 초에 큰 화제가 되었고, CVPR에서도 큰 관심을 받았던 논문이라고 들었습니다. 그만큼 다른 리뷰글들이 이미 많이 있지만, 저도 그 글들을 참고해서 한번 리뷰해보겠습니다:)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/2jBcBju.jpg&quot; alt=&quot;스크린샷 2019-07-31 오후 2.00.35&quot; /&gt;&lt;/p&gt;

&lt;p&gt;StyleGAN을 통해 생성된 이미지들(가짜 이미지들)을 보면, 굉장히 고해상도/고퀄리티의 이미지를 볼 수 있습니다. 이 논문에서는 기존에 Traditional한 네트워크에서 여러가지를 개선하여서 더 좋은 이미지 생성을 가능하게 했습니다. 또한, 이전에는 가능하지 않았던 “scale-specific control”을 가능하게 했습니다.&lt;/p&gt;

&lt;p&gt;이 논문의 특징들은 다음과 같이 크게 3가지로 정리할 수 있습니다.&lt;/p&gt;

&lt;form&gt;
  &lt;fieldset&gt;
  &lt;div&gt;1. scale-specific control of the synthesis&lt;/div&gt;&lt;br /&gt;
  &lt;div&gt;2. separation of high-level attributes and stochastic variation&lt;/div&gt;&lt;br /&gt;
  &lt;div&gt;3. new metrics: perceptual path length, linear separability&lt;/div&gt;
  &lt;/fieldset&gt;
&lt;/form&gt;

&lt;p&gt;그럼, 어떠한 방법을 이용했는지 한번 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;style-based-generator&quot;&gt;Style-based generator&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Y9QBdCV.png&quot; alt=&quot;무제.png.001&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 그림은 논문에 실려있는 그림을 가져온 것입니다. 왼쪽이 traditional network (progressive GAN인 것 같습니다), 오른쪽이 이 논문에서 제안한 Style-gased generator 입니다. 왼쪽 네트워크와 오른쪽에 Synthesis Network가 똑같은 구조를 갖고 있지만, 이전 GAN에서는 latent z를 바로 input으로 넣어줬던 것과는 다르게, StyleGAN에서는 학습된 constant 값을 넣어줍니다. 또한, 새롭게 Mapping Network와 Noise가 추가된 것을 볼 수 있습니다.&lt;/p&gt;

&lt;form class=&quot;notice--primary&quot;&gt;
  &lt;fieldset&gt;
  &lt;div&gt;1. Mapping Network가 새롭게 생김...z에서 w를 매핑한다?&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;
  &lt;div&gt;2. Synthesis Network? AdaIN, style ... 에 대해서&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;
  &lt;div&gt;3. Noise는 뭘 뜻하는가?&lt;/div&gt;
  &lt;/fieldset&gt;
&lt;/form&gt;

&lt;h3 id=&quot;1-mapping-network&quot;&gt;1. Mapping Network&lt;/h3&gt;
&lt;p&gt;StyleGAN에서는 Latent z를 곧바로 Synthesis Network에 넣어주는 것이 아니라, Mapping Network를 통해 w를 매핑한 후, w를 넣어줍니다. 논문에서는 이것에 대한 이유를 disentanglement라고 설명합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/tMan6dt.png&quot; alt=&quot;무제&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 Disentanglement에 관하여 논문에 실린 그림입니다. 여기서 Disentanglement를 이 논문에서는&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“latent space that consists of linear subspaces, each of which controls one factor of variation”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 정의합니다.&lt;/p&gt;

&lt;p&gt;잘 와닿지 않아서, &lt;a href=&quot;https://jayhey.github.io/deep%20learning/2019/01/14/style_based_GAN_1/&quot;&gt;Jaeyun’s Blog&lt;/a&gt;를 참고하였습니다. 예를 들어 z의 특정한 값을 바꿨을 때 생성되는 이미지의 하나의 특성(성별, 머리카락의 길이, 바라보는 방향 등)만 영향을 주게 되면 disentanglement라고 합니다.&lt;/p&gt;

&lt;p&gt;원래 기존의 네트워크에서는 z를 바로 input으로 넣어주는데, 그러면 고정된 z의 분포를 training set 분포에 맞추려고 하기 때문에 disentangled 하지 못하게 됩니다. 위 그림에서도 볼 수 있듯이 만약 training set에 missing combination이 있다면, Z는 이러한 이상한 combination이 나오지 않도록 하기 위해 curved 되서 entangled하게 되죠.&lt;/p&gt;

&lt;p&gt;하지만, W를 feature에 매핑하는 경우에는 다릅니다. W는 Z처럼 고정된 분포를 따르지 않습니다. sampling density는 학습된 piecewise continuous mapping f(z) (f는 mapping network 입니다)에 의해 정해지게 됩니다. 따라서, warping(틀어짐)이 많이 일어나지 않습니다. 그렇기 때문에 factors of variation은 더욱 linear하고, disentangled 하다고 할 수 있습니다. 이것이 바로 z를 곧바로 feature에 매핑하는 것보다 w에 매핑하는 것의 장점입니다.&lt;/p&gt;

&lt;h3 id=&quot;2-synthesis-network&quot;&gt;2. Synthesis Network&lt;/h3&gt;
&lt;p&gt;z를 중간 latent space W에 매핑을 한 뒤에 이 w는 “A”를 거쳐서 style, &lt;script type=&quot;math/tex&quot;&gt;y = (y_s, y_b)&lt;/script&gt;로 변형됩니다. 이때 A는 학습된 affine transform 입니다. 그리고 이 style들은 AdaIN(adaptive instance normalization) opeartion을 control 합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AdaIN(x_i, y) = y_{s,i}{ x_i - \mu(x_i) \over \sigma(x_i)}+ y_{b,i}&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.06868.pdf&quot;&gt;AdaIN&lt;/a&gt;은 style transfer를 할 때 많이 쓰이는 방법으로, 임의의 style transfer를 실시간으로 가능하게 합니다. (나중에 이 논문도 읽어봐야겠네요.) 여기서 feature map &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;는 normalized 된 다음에, style로 변환된 두 y로 scaled, biased 됩니다. style이 입혀지는 거죠. 이 과정을 매 layer 마다 반복합니다. 그리고 이러한 방법은 &lt;strong&gt;scale-specific control&lt;/strong&gt; 을 가능하게 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/0zicfnK.jpg&quot; alt=&quot;무제&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 각 layer 마다 다른 style 조절이 가능합니다. (&lt;script type=&quot;math/tex&quot;&gt;4^2 - 8^2&lt;/script&gt;) 에서는 coarse style (pose, hair style, face shape 등) (&lt;script type=&quot;math/tex&quot;&gt;16^2 - 32^2&lt;/script&gt;)에서는 middle style (facial features, hair style, eyes open/closed 등) (&lt;script type=&quot;math/tex&quot;&gt;64^2 - 1024^2&lt;/script&gt;) 에서는 fine style (color scheme) 등을 나타내서 각 부분에 다른 w를 넣어서 style 조절을 가능하게 하죠.&lt;/p&gt;

&lt;p&gt;하지만, style 들이 localized 될 수 있도록 하는 거엔 AdaIN 만으론 부족하고, 여기서는 &lt;strong&gt;Style Mixing&lt;/strong&gt; 이라는 방법도 사용합니다. 간단히 설명하자면, mixing regularization이라고 할 수 있는데, training 때 z를 하나가 아닌 두 개의 z를 넣어주는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;3-noise&quot;&gt;3. Noise&lt;/h3&gt;
&lt;p&gt;위에 Synthesis Network에서 결정하는 것이 high-level 특성들이라고 한다면, 이 noise를 통해 결정하는 것은 stochastic variation 입니다. stochastic variation은 머리의 세세한 결, 콧수염, 주근깨와 같이 perception of the image에 영향을 주지 않고, randomized 될 수 있는 부분들을 말합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/4m8e7lF.jpg&quot; alt=&quot;무제&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽 그림처럼 stochastic variation에 따라서 머리카락의 모양이 미세하게 달라지는 것을 볼 수 있습니다. 또한, 오른쪽 그림처럼 각 layer에 noise를 넣었는지 넣지 않았는지 여부에 따라서도 머리카락 모양의 차이가 생깁니다.&lt;/p&gt;

&lt;p&gt;만약 둘을 같은 latent codes z 로 나타내려고 한다면, high-level 특성들은 각 픽셀마다 똑같이 변화해야 하는데, stochastic variation 의 경우에는 픽셀별로 다르게 나타나야하기 때문에 잘 표현이 되지 않겠죠. high-level 특성들과 이러한 stochastic variation을 따로 구분하면, per-pixel noise가 가능하게 되고, 더욱 세세한 표현을 할 수 있게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;new-metrics&quot;&gt;New metrics&lt;/h2&gt;
&lt;p&gt;이 논문에서는 disentanglement를 측정하는 metric 두 가지를 새롭게 제안합니다. Perceptual path length 와 Linear separability 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/yQadcwb.png&quot; alt=&quot;무제&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-perceptual-path-length&quot;&gt;1. Perceptual path length&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;measure how drastic changes the image undergoes as we perform interpolation in the latent space&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;latent space에서 interpolation을 했을 때, 얼마나 큰 변화가 있는지 측정하는 것입니다. 왜 interpolation을 해서 측정을 하냐면, interpolation을 했을 때 일어나는 변화는 disentanglement와 관련이 있기 때문입니다. 예를 들어, interpolation을 했을 때 non-linear한 변화가 이미지에서 일어난다면, latent space가 entangled 할 수 있다는 것이죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/AjfqsA7.png&quot; alt=&quot;무제&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것을 측정하기 위하여 논문에서는 “perceptually-based pairwise image distance”를 구합니다. 이것의 식은 위와 같습니다. 각각 z와 w 일 경우의 distance를 구하는 공식이고, z는 spherical interpolation을 하고, w는 linear interpolation을 하는데, W의 벡터들은 normalized 되어있지 않기 때문입니다.&lt;/p&gt;

&lt;h3 id=&quot;2-linear-separability&quot;&gt;2. Linear separability&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;measure how well the latent-space points can be separated into two distinct sets via a linear hyperplane&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;latent space가 충분히 disentangled 하다면, 각각의 factors of variation에 해당하는 방향 벡터를 찾을 수 있어야 한다고 합니다. 따라서 latent-space 점들이 linear hyperplane으로 두개로 잘 구분이 되는지를 측정합니다. 이때, 40개의 특성을 갖는 데이터 셋에서 auxiliary classification network를 이용하여 SVM으로 binary classification을 하고, 예측된 class 를 따져서 conditional entropy &lt;script type=&quot;math/tex&quot;&gt;H(Y \mid X)&lt;/script&gt;를 구합니다. 이때, X는 SVM으로 예측된 class를, Y는 pre-trained classifier로 예측된 class를 말합니다.&lt;/p&gt;

&lt;p&gt;이를 이용해 sample이 true class를 결정하기 위해 additional information이 얼마나 필요한지 알 수 있고, 이로 separability score를 계산하여 metric으로 사용합니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p style=&quot;font-size:14px; color:gray;&quot;&gt;
이렇게 저의 첫 논문 리뷰를 마쳤습니다. 이 논문을 읽으면서 더욱 GAN에 대해서 공부해보고 싶은 욕구가 생기는 동시에 부족함을 많이 느낍니다. 아직 여기서 나오는 WGAN, 여러 개념들에 대해서 잘 모르는 것들이 많은데, 더 공부해서 또 글을 써봐야겠습니다.
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;br /&gt;
[1] &lt;a href=&quot;https://arxiv.org/abs/1812.04948&quot;&gt;StyleGAN 논문&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://github.com/NVlabs/stylegan&quot;&gt;StyleGAN 코드&lt;/a&gt;&lt;br /&gt;
[3] &lt;a href=&quot;https://jayhey.github.io/deep%20learning/2019/01/14/style_based_GAN_1/&quot;&gt;Jaeyun’s Blog&lt;/a&gt;&lt;/p&gt;</content><author><name>ZZEN</name><email>nuguziii@naver.com</email></author><category term="GAN" /><summary type="html">이 글은 2019 CVPR 논문, A Style-Based Generator Architecture for Generative Adversarial Networks를 참고하여 작성하였습니다.</summary></entry></feed>