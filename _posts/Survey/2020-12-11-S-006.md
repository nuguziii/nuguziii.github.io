---
title : "Image Feature Learning - Self-Supervised Learning (2) _ Contrastive Learning"
categories :
  - Survey
tags :
  - Deep Learning
  - Image Feature Learning
  - Self-supervised Learning
  - Contrastive Learning
comments : true
---

Contrastive Learning 에 대한 개념 및 연구들을 정리한 글입니다.

![스크린샷 2020-12-06 오후 2.00.17](https://i.imgur.com/krTrbrA.png)

self-supervised learning 방법들을 위 그림과 같이 네가지로 분류해 보았고, [이전 글](https://nuguziii.github.io/survey/S-005/)에서 Generative, Predictive, Synthetic 방법들을 연구들과 함께 소개했습니다.

![스크린샷 2020-12-06 오후 2.14.51](https://i.imgur.com/6sRUwVD.png)

이전에 소개해드린 방법들도 high-level feature들을 학습할 수 있다는 것을 보여주었지만, 사실 정확도를 측정해보면 위 그림의 빨간색과 같이 그렇게 높지는 않습니다. 하지만 이 글에서 설명드릴 Contrastive 방법은 위 그림의 왼쪽 차트에서 상위에 분포할 만큼 이전 방법들에 비해 성능이 좋습니다.

이유를 생각해보면, loss를 정의하는 space가 다르기 때문입니다. generative나 predictive 방법들을 통해 high-level feature를 학습하기 위한 시도는 많았지만, 사실 우리는 정확히 모델이 무엇을 학습하는지는 모릅니다. 단지 이렇게 하면 더 잘 학습하지 않을까 예측할 뿐이죠.

Contrastive Learning은 loss를 representation space에서 직접 정의하기 때문에 더욱 직관적으로 우리가 무엇을 학습하는지 알 수 있습니다. 최근 연구들을 통해서 이 방법에 대해 더 자세히 설명드리도록 하겠습니다.

# Contrastive Learning 이란

> Contrastive self-supervised learning techniques are a promising class of methods that build representations by learning to encode what makes two things similar or different

Contrastive learning은 representation space에서 어떤 것들이 비슷하니 가까이 있어야하고, 어떤 것들이 다르니 멀리 있어야 한다는 것을 학습하는 과정을 통해 rich representation을 얻는 방법입니다.

예를 들어 classification task라고 생각했을때, 같은 강아지 이미지들은 representation이 가까이에 위치하고, 강아지와 고양이 이미지 사이에는 거리가 있어야 boundary가 잘 구분될 수 있습니다.

contrastive learning을 잘 이해하기 위해서는 instance discrimination, NCE(noise contrastive estimation), positive/negative sample, InfoNCE Loss 개념에 대해 이해할 수 있어야 합니다.

## Instance Discrimination

unsupervised learning에서는 label이 존재하지 않기 때문에 개와 고양이 이렇게 나눌 수가 없습니다. 따라서 저희는 instance discrimination을 합니다.

instance는 training 되는 각 이미지를 뜻합니다. 이미지 모두를 각각의 클래스로 생각해서 이미지가 N개면 N개의 classification을 수행합니다. 이렇게 학습한 representation으로 downstream task를 수행하면 supervised learning만큼 좋은 결과를 얻을 수 있다는 것이 증명되었습니다. ([참고](https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf))

## Noise Contrastive Estimation

Instance discrimination을 하기 때문에 저희는 N개의 이미지에 대해 N-classification을 합니다. 하지만, 이미지의 개수가 1억장이면 1억개의 classification을 하는 것은 불가능 하기 때문에, 우리는 approximate 합니다.

positive, negative 인 binary classification으로 생각을 하고, 임의의 sample 안에서 positive와 negative 클래스만 구별을 해 전체를 approximate 합니다.

## Positive, Negative Samples

이제는 N개의 multi-classification 문제가 binary classification 문제로 좁혀졌습니다. 여기서 중요한 것은 positive, negative sample을 어떻게 정의하는지 입니다.

![data_augmentation](https://miro.medium.com/max/2000/1*rvwzKkvhlDN3Wo_4Oay_4Q.png)

저희는 positive sample을 정의하기 위해 data augmentation 방법을 사용합니다. 우리 사람은 고양이 이미지를 회전하던, 컬러를 바꾸던 똑같이 고양이로 인식할 수 있습니다. 마찬가지로 고양이 이미지와 고양이 이미지를 transformation한 이미지 둘 다 representation이 비슷해야 하기 때문에 이 둘을 positive sample이라고 합니다. 아닌 것은 모두 negative sample 입니다.

## InfoNCE Loss

positive sample 끼리는 가깝게, negative sample 끼리는 멀게 representation을 정의하기 위해서 보통 InfoNCE Loss를 사용합니다.

![스크린샷 2020-12-11 오후 6.31.56](https://i.imgur.com/FDrdxiy.png)

InfoNCE Loss는 위 그림에 $$L_N$$ 과 같습니다. 초록색은 positive sample과의 관계, 빨간색은 negative sample과의 관계를 의미합니다. InfoNCE Loss를 minimize하면 분자(초록색)는 minimize되고, 분모(빨간색)는 maximize되기 때문에 우리가 원하는 결과를 얻을 수 있습니다.

위 그림에서 score는 보통 cosine similarity를 사용합니다. InfoNCE Loss에서도 마찬가지로 cosine similarity가 사용됩니다.

다음 loss를 optimize 하면 위에 왼쪽 위 그림처럼 augmentation을 한 같은 이미지끼리는 Positive sample로 거리가 가깝고, 다른 이미지 끼리는 negative sample로 거리가 먼 형태의 representation이 형성됩니다.

---

사실 여기에서는 제가 positive sample은 data augmentation 방법을 사용하여서, contrastive loss는 InfoNCE Loss를 사용해서 정의를 했지만, 둘을 어떻게 정의하는지는 달라질 수 있습니다. 여기서 중요한 것은 semantically similar view는 가깝게, semantically different view는 멀게 latent representation에 encode 하는 것입니다. 이에 대한 정의는 위와 달라질 수 있습니다.

# SimCLR, MoCov2, BYOL

2020년에 나온 contrastive learning 방법을 적용한 세 논문을 소개합니다. 다른 contrastive learning 방법에 비해 압도적으로 세 방법을 이용한 성능이 좋기도 하고, 가장 나중에 나온 BYOL 방법의 경우 간단한 방법으로 supervised learning의 성능과 거의 비슷한 성능을 보이기도 했습니다.

## SimCLR & MoCov2

![스크린샷 2020-12-11 오후 6.36.37](https://i.imgur.com/iK7vFXe.png)

일단, **SimCLR** 부터 설명드리자면, 위 그림의 (a)와 같은 구조를 갖고 있습니다. 왼쪽에 개와 의자 이미지와 같이 한 이미지에 data augmentation을 적용해 2개의 positive sample들을 만듭니다. (data augmentation의 경우 random crop과 color distortion 조합이 가장 효과적이었다고 합니다)

왼쪽 encoder에는 positive sample pair, 오른쪽 encoder에는 negative sample pair가 들어가 각각의 representation이 출력됩니다. 이를 MLP layer에 통과시킨 다음 InfoNCE Loss를 적용합니다.

한 batch에 들어가는 모든 이미지에 대해 positive pair와 negative pair similarity를 계산합니다. [CPC 논문](https://arxiv.org/pdf/1807.03748.pdf)에 의하면 InfoNCE Loss에서 계산되는 negative sample의 개수가 많을 수록 representation 성능이 좋아진다고 합니다. 따라서 SimCLR에서도 배치 사이즈를 거의 10000에 가깝게 했을 때 가장 좋은 성능을 보입니다. (이 방법에서 negative sample size는 batch size에 비례합니다 - 구글에서 쓴 논문이기에 가능했던 것 같습니다)

하지만, 일반 사람들이 실험을 할 때, 배치 사이즈를 SimCLR와 같이 크게 하는 것은 불가능합니다. 따라서 이로 인한 문제를 해결한 방법이 **MoCo** 논문입니다.

data augmentation, loss, 모델 구조는 모두 SimCLR와 같은데 다른점은 MoCo에서는 momentum encoder를 사용하고, queue 구조를 갖는 dictionary를 사용한다는 것입니다.

MoCo에서는 양쪽 encoder로 모두 positive sample들만 들어갑니다. negative sample은 queue에 따로 저장을 해서 불러옵니다. InfoNCE Loss는 encoder를 통과한 positive pair 사이의 similarity 그리고 queue에서 불러온 negative pair 사이의 similarity를 계산하여 구해집니다.

![스크린샷 2020-12-11 오후 8.43.16](https://i.imgur.com/RGNhwJw.png)

첫번째 step에서 positive sample로 들어가서 encoder를 통과하여 구해진 representation은 다음 step 에서 queue에 들어가서 negative sample로써 계산됩니다. queue size가 있어서 사이즈가 다 찼을 때는, 가장 오래 있었던 representation이 빠지고 현재 representation이 새로 queue에 들어옵니다.

이 방법으로 negative sample이 batch size에 비례하지 않아도 가능하게 되었습니다. 근데, 이 방법으로는 negative sample에 대해 momentum encoder를 학습할 수 없습니다. 따라서 MoCo에서는 momentum encoder를 encoder의 파라미터를 조금씩 입혀가는 방식으로 학습합니다.

위에 수도코드를 참고하면 더 자세히 알 수 있을 것 같습니다. 이와 같은 방법으로 MoCov2는 SimCLR에 비해 훨씬 작은 batch size로 더 좋은 성능을 냈습니다.

## Bootstrap Your Own Latent A New Approach to Self-Supervised Learning (BYOL)

negative sample이 많을 수록 성능이 좋아지기 때문에 MoCo에서는 momentum encoder라는 복잡한 방법을 사용하면서까지 negative sample이 많이 반영될 수 있도록 했습니다. 하지만 BYOL에서는 더이상 negative sample이 없이도 더 좋은 성능을 내는 것을 보여줬습니다.

BYOL 논문을 설명하기 전에 왜 negative sample이 필요한지에 대해 설명하려고 합니다. 사실 positive sample 끼리는 가까워야한다는 조건만 갖고도 representation을 잘 학습할 수 있을 것 같지만, 막상 학습하게 되면 **collapsed representation**을 얻게 됩니다.

collapsed representation은 모델이 어떠한 input에 대해서도 같은 값을 내보내서 생기는 현상인데, positive sample 만 학습할 경우 결국 모델은 loss를 0으로 만들기 위해 모두 같은 값을 내보내면 되기 때문에 학습이 되지 않습니다. 이를 방지하기 위해 negative sample들을 넣어주는 것이죠.

하지만, BYOL에서는 negative sample 없이 positive sample 만 학습해도 collapsed representation 현상이 일어나지 않습니다. 어떤 방법을 사용하였는지 한번 알아보겠습니다.

![스크린샷 2020-12-11 오후 8.50.07](https://i.imgur.com/6d1zgQl.png)

모델의 형태는 위 그림과 같습니다. 매우 단순하죠. 심지어 Loss도 L2 loss를 사용합니다. 이 논문에서는 online network, target network 두개를 정의합니다. Mean Teacher와 거의 구조나 학습 방식이 같습니다. Mean Teacher에 대해 궁금하신 분들은 [이 글](https://nuguziii.github.io/paper-review/PR-009/)을 참고바랍니다.

online network는 prediction을 하고, target network의 output을 갖고 학습을 합니다. target network는 exponential moving average 방법으로 online network의 파라미터로 업데이트 됩니다. 여기서 Mean Teacher 방법과 한가지 다른 점은 online network에서 target network와 같은 구조를 사용한 것이 아니라 뒤에 MLP layer를 추가하여 prediction을 한 것입니다.

이 모델에서 prediction layer를 따로 사용한 점, exponential moving average를 한 점으로 collapsed representation이 일어나지 않았다고 합니다. prediction layer를 없애면 바로 collapsed representation이 생깁니다. 이것이 어떻게 가능한지에 대해서는 [이 논문](https://arxiv.org/abs/2011.10944)을 참고해도 좋을 것 같습니다.

이 방법으로 SimCLR, MoCo의 성능을 뛰어넘은 것은 물론 supervised learning 방법과 거의 근사한 성능을 보였습니다.

---

**Reference:**<br>
[1] [Contrastive Self-Supervised Learning - Ankesh Anand](https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html)<br>
[2] [MoCo](https://arxiv.org/pdf/2003.04297.pdf)<br>
[3] [BYOL](https://arxiv.org/pdf/2006.07733.pdf)<br>
